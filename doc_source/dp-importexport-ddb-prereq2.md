# Before You Begin<a name="dp-importexport-ddb-prereq2"></a>

You must complete part one of this tutorial to ensure that your DynamoDB table contains the necessary data to perform the steps in this section\. For more information, see [Part One: Import Data into DynamoDB](dp-importexport-ddb-part1.md)\. 

Additionally, be sure you've completed the following steps: 
+ Complete the tasks in [Setting up for AWS Data Pipeline](dp-get-setup.md)\.
+ Create a topic and subscribe to receive notifications from AWS Data Pipeline regarding the status of your pipeline components\. For more information, see [Create a Topic](https://docs.aws.amazon.com/sns/latest/gsg/CreateTopic.html) in the *Amazon SNS Getting Started Guide*\.
+ Ensure that you have the DynamoDB table that was created and populated with data in part one of this tutorial\. This table will be your data source for part two of the tutorial\. For more information, see [Part One: Import Data into DynamoDB](dp-importexport-ddb-part1.md)\. 

Be aware of the following:
+ Imports may overwrite data in your DynamoDB table\. When you import data from Amazon S3, the import may overwrite items in your DynamoDB table\. Make sure that you are importing the right data and into the right table\. Be careful not to accidentally set up a recurring import pipeline that will import the same data multiple times\.
+ Exports may overwrite data in your Amazon S3 bucket\. When you export data to Amazon S3, you may overwrite previous exports if you write to the same bucket path\. The default behavior of the **Export DynamoDB to S3** template will append the job's scheduled time to the Amazon S3 bucket path, which will help you avoid this problem\.
+ Import and Export jobs will consume some of your DynamoDB table's provisioned throughput capacity\. This section explains how to schedule an import or export job using Amazon EMR\. The Amazon EMR cluster will consume some read capacity during exports or write capacity during imports\. You can control the percentage of the provisioned capacity that the import/export jobs consume by with the settings M`yImportJob.myDynamoDBWriteThroughputRatio` and `MyExportJob.myDynamoDBReadThroughputRatio`\. Be aware that these settings determine how much capacity to consume at the beginning of the import/export process and will not adapt in real time if you change your table's provisioned capacity in the middle of the process\.
+ Be aware of the costs\. AWS Data Pipeline manages the import/export process for you, but you still pay for the underlying AWS services that are being used\. The import and export pipelines will create Amazon EMR clusters to read and write data and there are per\-instance charges for each node in the cluster\. You can read more about the details of [Amazon EMR Pricing](https://aws.amazon.com/elasticmapreduce/pricing/)\. The default cluster configuration is one m1\.small instance master node and one m1\.xlarge instance task node, though you can change this configuration in the pipeline definition\. There are also charges for AWS Data Pipeline\. For more information, see [AWS Data Pipeline Pricing](https://aws.amazon.com/datapipeline/pricing/) and [Amazon S3 Pricing](https://aws.amazon.com/s3/pricing/)\.
+ If a table is configured for On\-Demand Capacity, change the table back to provisioned capacity before running the export or import operations\. On\-Demand Capacity removes provisioned throughput and the pipeline will need a throughput ratio to calculate resources to use from the DynamoDB table\. You may use CloudWatch metrics to evaluate the aggregate of throughput the table has used and provision the throughput capacity accordingly\.